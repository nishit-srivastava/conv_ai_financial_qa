{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdf52a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conv_ai_financial_qa\\conv_ai_financial_qa\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07801e5a-8501-4761-a7c5-0dd64871ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loaded 837 metadata chunks\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Load FAISS + Metadata\n",
    "# -------------------------------\n",
    "index = faiss.read_index(\"../data/rag_chunks/faiss_index.idx\")\n",
    "\n",
    "with open(\"../data/rag_chunks/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“‚ Loaded {len(chunks)} metadata chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c056a12-3d8d-4de8-9e2b-24acf62ff2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Embedding Model (same as before)\n",
    "# -------------------------------\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f1025d-c659-44af-91fa-e448756e2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Retrieval Function\n",
    "# -------------------------------\n",
    "def search(query, top_k=5):\n",
    "    query_vec = embedder.encode([query]).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return [chunks[idx][\"text\"] for idx in I[0] if idx != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a316af99-b80e-41b4-b776-37b3cb2e1fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# ---- Model\n",
    "model_name = \"google/flan-t5-base\"   # try flan-t5-large if CPU allows\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "llm = pipeline(\"text2text-generation\", model=llm_model, tokenizer=tokenizer)\n",
    "\n",
    "STOPWORDS = set(\"\"\"\n",
    "a an and are as at be by for from has have in is it of on or that the to was were will with\n",
    "fy q1 q2 q3 q4 year quarter\n",
    "\"\"\".split())\n",
    "\n",
    "def count_tokens(txt: str) -> int:\n",
    "    return len(tokenizer.encode(txt, add_special_tokens=True))\n",
    "\n",
    "def simple_overlap_score(q: str, p: str) -> int:\n",
    "    # naive keyword overlap to push relevant passages up\n",
    "    tok = lambda s: [w for w in re.findall(r\"[A-Za-z0-9,.-]+\", s.lower()) if w not in STOPWORDS]\n",
    "    qset = set(tok(q))\n",
    "    return sum(1 for w in tok(p) if w in qset)\n",
    "\n",
    "def search_with_text(question: str, top_k: int = 12):\n",
    "    \"\"\"\n",
    "    Uses your FAISS index + embedder defined earlier in the notebook.\n",
    "    Returns (passages, distances).\n",
    "    \"\"\"\n",
    "    qv = embedder.encode([question]).astype(\"float32\")\n",
    "    D, I = index.search(qv, top_k)\n",
    "    hits = []\n",
    "    for rank, idx in enumerate(I[0]):\n",
    "        if idx == -1: \n",
    "            continue\n",
    "        hits.append((chunks[idx][\"text\"], float(D[0][rank])))\n",
    "    return hits  # list of (text, distance)\n",
    "\n",
    "def build_prompt(question: str, passages: list[str], token_budget: int):\n",
    "    system = (\n",
    "        \"You are a financial analyst assistant.\\n\"\n",
    "        \"Use ONLY the provided context to answer. If the answer is not present, say you don't know.\\n\"\n",
    "        \"Cite the period identifiers (e.g., Q4 FY24, FY23) when relevant.\\n\\n\"\n",
    "    )\n",
    "    prefix = system + \"Context:\\n\"\n",
    "    suffix = f\"\\n\\nQuestion: {question}\\nAnswer in 1â€“2 complete sentences:\\n\"\n",
    "\n",
    "    selected = []\n",
    "    for p in passages:\n",
    "        trial = prefix + \"\\n\".join(selected + [p]) + suffix\n",
    "        if count_tokens(trial) > token_budget:\n",
    "            continue\n",
    "        selected.append(p)\n",
    "\n",
    "    # if nothing fit (very long first passage), hard-truncate first\n",
    "    if not selected and passages:\n",
    "        p = passages[0]\n",
    "        while p and count_tokens(prefix + p + suffix) > token_budget:\n",
    "            p = p[: max(32, len(p)//2)]\n",
    "        if p: selected = [p]\n",
    "\n",
    "    return prefix + \"\\n\".join(f\"- {s}\" for s in selected) + suffix\n",
    "\n",
    "def rag_query(question: str, top_k_retrieval: int = 12, max_ctx_passages: int = 6,\n",
    "              input_token_budget: int = 480, max_new_tokens: int = 96):\n",
    "    # 1) Retrieve\n",
    "    hits = search_with_text(question, top_k=top_k_retrieval)\n",
    "    passages = [t for (t, _) in hits]\n",
    "\n",
    "    # 2) Re-rank by keyword overlap (keeps FAISS order as tie-breaker)\n",
    "    passages = sorted(passages, key=lambda p: simple_overlap_score(question, p), reverse=True)\n",
    "    passages = passages[:max_ctx_passages]\n",
    "\n",
    "    # 3) Build prompt within budget (Flan-T5 <= 512)\n",
    "    prompt = build_prompt(question, passages, token_budget=input_token_budget)\n",
    "\n",
    "    # 4) Generate (encourage complete answers)\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_new_tokens=24,              # <-- avoid one-word answers\n",
    "        do_sample=False,\n",
    "        num_beams=4,                    # better completeness on CPU\n",
    "        no_repeat_ngram_size=3,\n",
    "        truncation=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    return out[0][\"generated_text\"].strip(), passages, count_tokens(prompt)\n",
    "\n",
    "def debug_query(q):\n",
    "    ans, used_passages, tok = rag_query(q)\n",
    "    print(f\"Question: {q}\")\n",
    "    #print(f\"Input tokens: {tok} | Passages used: {len(used_passages)}\")\n",
    "    print(\"\\n--- Context used ---\")\n",
    "    #for i, p in enumerate(used_passages, 1):\n",
    "        #print(f\"[{i}] {p}\\n\")\n",
    "    print(\"--- Answer ---\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3bb2fb0-1c68-4d43-9c5a-7bce03dd4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the revenue from operations in Q4 FY24?\n",
      "\n",
      "--- Context used ---\n",
      "--- Answer ---\n",
      "51,488. (Source: financial_statement_fixed_2024) Revenue from operations in Q3 FY23 was 50,844.\n"
     ]
    }
   ],
   "source": [
    "debug_query(\"What was the revenue from operations in Q4 FY24?\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5591f36-0c8a-4edd-ac8e-464ca8ee6da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_ai_env",
   "language": "python",
   "name": "conv_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
